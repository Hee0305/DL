{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "FT",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "3834ce636a3ba6c6c2bd8b9b527c48eede78c367f849f6cce666ea7f1d26e2fb"
    },
    "kernelspec": {
      "display_name": "Python 3.8.5 64-bit ('base': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hee0305/DL/blob/main/FT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCSmeAePjsa3"
      },
      "source": [
        "# Transfer Learning and Visualization (CNN's)\n",
        "- cifar-10\n",
        "- resnet50\n",
        "- mobilenetV2\n",
        "\n",
        "> https://medium.com/@andrew.dabydeen/transfer-learning-using-resnet50-and-cifar-10-6242ed4b4245"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDZO46ovqQSp"
      },
      "source": [
        "### Transfer learning (imagenet)\n",
        "- ImageNet dataset \n",
        "    - 1.4 million labeled images \n",
        "    - 1,000 different classes  \n",
        "    \n",
        "tensorflow.keras.applications\n",
        "- Xception\n",
        "- Inception V3\n",
        "- ResNet50\n",
        "- VGG16\n",
        "- VGG19\n",
        "- MobileNet\n",
        "\n",
        "> [ref1-architecture-comparison-of-alexnet-vggnet-resnet-inception-densenet](https://towardsdatascience.com/architecture-comparison-of-alexnet-vggnet-resnet-inception-densenet-beb8b116866d)\n",
        "\n",
        "> [ref2-imagenet-vggnet-resnet-inception-xception-keras](https://www.pyimagesearch.com/2017/03/20/imagenet-vggnet-resnet-inception-xception-keras/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Jz-FYp7jqNf"
      },
      "source": [
        "This Colab notebook demonstrates transfer learning with a pretrained ConvNet: MobileNetV2\n",
        "\n",
        "0.   Data pipeline\n",
        "1.   Baseline model: train a simple CNN from scratch\n",
        "2.   Transfer learning: pretraiend ConvNet as a feature extractor\n",
        "3.   Transfer learning: fine-tune a pretrained ConvNet\n",
        "4.   Test accuracy & visualize predictions\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AizJV_s0jsa3"
      },
      "source": [
        "Before beginning, let's load the appropriate libraries needed for this notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ad-4gap-Ln0L"
      },
      "source": [
        "# ConvNet: MobileNetV2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Y2zSloxjsa3",
        "outputId": "1f8ad288-8688-4201-f764-6d0f806a855f"
      },
      "source": [
        "# from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
        "from tensorflow.keras.applications.mobilenet_v2 import  MobileNetV2, preprocess_input\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import optimizers\n",
        "import tensorflow as tf\n",
        "from keras.utils import np_utils\n",
        "from keras.models import load_model\n",
        "from keras.datasets import cifar10\n",
        "from keras.preprocessing import image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "# import cv2\n",
        "print(tf.__version__)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJckUaE1T-DX",
        "outputId": "f2fa407c-cd9b-46a4-8b74-cf316edc87d8"
      },
      "source": [
        "tf.keras.backend.clear_session()  # For memory\n",
        "\n",
        "# GPU check\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
        "        print(\"#### \", len(gpus), \"Physical GPUs,\",\n",
        "                 len(logical_gpus), \"Logical GPUs\")\n",
        "    except RuntimeError as e:\n",
        "        # Memory growth must be set before GPUs have been initialized\n",
        "        print(e)\n",
        "else:\n",
        "    print('#### No CUDA supported GPU in this computer.')"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "####  1 Physical GPUs, 1 Logical GPUs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFs84Y54W2cs"
      },
      "source": [
        "## Load data \n",
        "- Use total data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBHPOuWity6c",
        "outputId": "f5c6bf3d-314b-4f7f-ace7-e24432fb10d3"
      },
      "source": [
        "from keras.datasets import cifar10\n",
        "# load dataset\n",
        "(trainX, trainy), (testX, testy) = cifar10.load_data()\n",
        "# summarize loaded dataset\n",
        "print('Train: X=%s, y=%s' % (trainX.shape, trainy.shape))\n",
        "print('Test: X=%s, y=%s' % (testX.shape, testy.shape))\n",
        "\n",
        "#training의 10% split -> numpy 코드 이용(사이킷런)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: X=(50000, 32, 32, 3), y=(50000, 1)\n",
            "Test: X=(10000, 32, 32, 3), y=(10000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvGeEoc5VMPv"
      },
      "source": [
        "cifar 10% -- 5000/1000"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zud97VptVJkk",
        "outputId": "5cc9005d-2a59-4e04-cd0a-38bef822bf7d"
      },
      "source": [
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test = \\\n",
        "            train_test_split(trainX,trainy,\n",
        "                             random_state=42,\n",
        "                             test_size=0.1,\n",
        "                             stratify=trainy)\n",
        "\n",
        "Xtrain = X_test\n",
        "print(Xtrain.shape) # X_train\n",
        "ytrain = y_test\n",
        "print(ytrain.shape) # y_train\n",
        "\n",
        "\n",
        "X_train,X_test,y_train,y_test = \\\n",
        "            train_test_split(testX,testy,\n",
        "                             random_state=42,\n",
        "                             test_size=0.1,\n",
        "                             stratify=testy)\n",
        "\n",
        "print(X_test.shape) # X_test\n",
        "print(y_test.shape) # y_test"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5000, 32, 32, 3)\n",
            "(5000, 1)\n",
            "(1000, 32, 32, 3)\n",
            "(1000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikSSR60vjsa4"
      },
      "source": [
        "## *Preprocess the dataset*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIxdiJVKArC6",
        "trusted": true
      },
      "source": [
        "def preprocess_image_input(input_images): #이미지 픽셀 값 정수 -> 실수, preprocess_input함수로 바꿔줌(정규화)\n",
        "  input_images = input_images.astype('float32')\n",
        "  output_ims = preprocess_input(input_images)\n",
        "  return output_ims"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOqjKzgAEU-Z",
        "trusted": true
      },
      "source": [
        "#전처리 된 데이터\n",
        "x_train = preprocess_image_input(Xtrain)\n",
        "x_test = preprocess_image_input(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bejNwNw-R1Ut"
      },
      "source": [
        "x_train.shape ,X_test.shape #,len(x_train),x_train[4].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_OJFy4HKL3t"
      },
      "source": [
        "y_train=ytrain\n",
        "y_test=y_test\n",
        "y_train.shape ,y_test.shape #레이블 구조: 2차원 행벡터"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akP3Ki7Xjmzb"
      },
      "source": [
        "augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HK5WSgOFjsa4"
      },
      "source": [
        "#### Model Creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0X-WBTpjsa4"
      },
      "source": [
        "Next, let's load MobileNetV2 with just the convolutional layers and not the dense layers so we can train our new dataset on the new dense layers that we create"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ib1aAiB8jsa4"
      },
      "source": [
        "TL_base = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3)) #imagenet: 1000개의 클래스, 140만개의 이미지\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aD-8N-FrNKe"
      },
      "source": [
        "len(TL_base.layers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyM_C3BPKL3s"
      },
      "source": [
        "# names of TL_base\n",
        "print([x.name for x in TL_base.layers],end=' ') #layer 이름 출력"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gPbJBfcETkf"
      },
      "source": [
        "### Check the start of trainable layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hGnViHWKL3s"
      },
      "source": [
        "TL_base.layers[81].name, TL_base.layers[107].name # mobilenetV2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNAlquF7jsa4"
      },
      "source": [
        "Let's get an idea on how the ResNet architecture looks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1-or3WPjsa4",
        "scrolled": false
      },
      "source": [
        "TL_base.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1rjNb94Nzpv"
      },
      "source": [
        "TL_base.layers[-1].get_config() # (7, 7, 1280), 맨 마지막 pooling 통과한 결과, 맨 마지막 index의 구조"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlDLhk6Ojsa4"
      },
      "source": [
        "For the new dataset, let's work with the cifar10 dataset which we can load directly from keras' dataset library. The Cifar10 data description is as follows - \"The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDHerLDLNzpx"
      },
      "source": [
        "## Sequential model\n",
        "- TL base: resnet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptRtmycONzpx"
      },
      "source": [
        "# Set TL_base trainable or not.\n",
        "TL_base.trainable = False  # Use the representative features pretrained by resnet."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhZf9L-ijsa4"
      },
      "source": [
        "model = models.Sequential()\n",
        "model.add(keras.Input(shape=(32,32,3))) #image: 224*224*3\n",
        "model.add(layers.UpSampling2D((7,7)))  # 32*7 = 224 #가로, 세로 각각 7배씩 => 크기 확대\n",
        "model.add(TL_base) #input을 TL_base에 넣어줌, frozen - convolution단계는 고정(기존 파라미터 이용)\n",
        "model.add(layers.GlobalAveragePooling2D()) # (7,7,2048) => (2048,) #1280개짜리 벡터 만들어짐\n",
        "model.add(layers.Flatten()) #fcn 설계\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Dense(1024, activation='relu')) #fcn1\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Dense(512, activation='relu')) #fcn2\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Dense(10, activation='softmax')) #output layer\n",
        "#fcn 3층 설계\n",
        "\n",
        "# model.compile(optimizer=optimizers.RMSprop(lr=2e-5), loss='binary_crossentropy', metrics=['acc'])\n",
        "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) \n",
        "model.compile(optimizer='SGD', \n",
        "                loss='sparse_categorical_crossentropy',  # sparse_categorical_crossentropy\n",
        "                metrics = ['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RS2RHGOty6-X"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UxfUsAqKL3u"
      },
      "source": [
        "# Plot model\n",
        "from tensorflow.keras.utils import plot_model\n",
        "plot_model(model, show_shapes=True, show_layer_names=True, to_file='model_S.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmnCEZ6mpeyG"
      },
      "source": [
        "from tensorflow.keras import layers, models, callbacks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OELMMUaRo9gu"
      },
      "source": [
        "#여기부터 중요\n",
        "#content에 model 폴더 생성 - 높아질때마다 model폴더에 생성\n",
        "mc_callback = callbacks.ModelCheckpoint(filepath=\"./model/cifar10_pct100_mobilenetV2S_SGD_best_weights.{epoch:03d}-{val_accuracy:.4f}.hdf5\", \n",
        "                              monitor='val_accuracy', verbose=0, save_best_only=True)\n",
        "es_callback = callbacks.EarlyStopping(monitor='val_accuracy', \n",
        "                            mode='max', verbose=1, patience=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IaGk31eYC0CJ"
      },
      "source": [
        "%%time\n",
        "history = model.fit(x_train, y_train, #5만장\n",
        "                    epochs=100, batch_size=50, \n",
        "                    validation_data=(x_test, y_test), #만장\n",
        "                    callbacks=[mc_callback,es_callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYb5sAEmk4ut"
      },
      "source": [
        "## Evaluate the Model\n",
        "\n",
        "Calculate the loss and accuracy metrics using the model's `.evaluate` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "io7Fuu-w3PZi",
        "trusted": true
      },
      "source": [
        "model.evaluate(x_test, y_test, batch_size=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOOsGxouNzpy"
      },
      "source": [
        "## 훈련 데이터와 검증 데이터에 대한 loss, accuracy 시각화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnXCRIh3Nzpz"
      },
      "source": [
        "# 훈련 데이터와 검증 데이터에 대한 loss 시각화.\n",
        "epochs = range(1, len(history.history['loss']) + 1)\n",
        "\n",
        "loss_list = history.history['loss'] #[100 * i for i in history.history['loss']]\n",
        "vloss_list = history.history['val_loss'] #[100 * i for i in history.history['val_loss']]\n",
        "\n",
        "plt.plot(epochs,loss_list)  \n",
        "plt.plot(epochs,vloss_list)\n",
        "\n",
        "plt.plot(np.argmin(np.array(vloss_list))+1,vloss_list[np.argmin(np.array(vloss_list))], 'r*')\n",
        "plt.title('cifar10_100%: TL(mobilenetV2) Sequential model - val_loss, min:' + str(np.round(vloss_list[np.argmin(np.array(vloss_list))],3)))\n",
        "plt.ylabel('val-Loss (%)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['loss','val_loss','best'], loc='best')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Q2vOfGGNzpz"
      },
      "source": [
        "# 훈련 데이터와 검증 데이터에 대한 accuracy 시각화.\n",
        "epochs = range(1, len(history.history['accuracy']) + 1)\n",
        "\n",
        "acc_list = [100 * i for i in history.history['accuracy']]\n",
        "vacc_list = [100 * i for i in history.history['val_accuracy']]\n",
        "\n",
        "plt.plot(epochs,acc_list)  \n",
        "plt.plot(epochs,vacc_list)\n",
        "\n",
        "plt.plot(np.argmax(np.array(vacc_list))+1,vacc_list[np.argmax(np.array(vacc_list))], 'r*')\n",
        "plt.title('cifar10_100%: TL(mobilenetV2) Sequential model - val_accuracy, max:' + str(np.round(vacc_list[np.argmax(np.array(vacc_list))],3)))\n",
        "plt.ylabel('val-Accuracy (%)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['accuracy','val_accuracy','best'], loc='best')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXzGyWjcjsa4"
      },
      "source": [
        "# model.save('cifar10_SGD_ep20acc88.hdf5')\n",
        "# # Load the saved model\n",
        "# model = tf.keras.models.load_model('cifar10_SGD_ep20acc88.hdf5')\n",
        "# model.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KVYc9Mujsa5"
      },
      "source": [
        "### Visualization loss & accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LI9ezMlWjsa5"
      },
      "source": [
        "The training/validation loss and accuracy visualizations are shown below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9EQ2Z_c2gYL"
      },
      "source": [
        "# More graphs of loss and accuracy\n",
        "history_dict = history.history\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(loss) + 1)\n",
        "\n",
        "plt.figure(figsize=(14, 4))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(epochs, loss, 'go-', label='Training Loss')\n",
        "plt.plot(epochs, val_loss, 'bd', label='Validation Loss')\n",
        "plt.plot(np.argmin(np.array(val_loss))+1,val_loss[np.argmin(np.array(val_loss))], 'r*', ms=12)\n",
        "plt.title('Training and Validation Loss, min: ' + str(np.round(val_loss[np.argmin(np.array(val_loss))],3)))\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "acc = history_dict['accuracy']\n",
        "val_acc = history_dict['val_accuracy']\n",
        "\n",
        "epochs = range(1, len(loss) + 1)\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(epochs, acc, 'go-', label='Training Accuracy') #, c='blue')\n",
        "plt.plot(epochs, val_acc, 'bd', label='Validation Accuracy') #, c='red')\n",
        "plt.plot(np.argmax(np.array(val_acc))+1,val_acc[np.argmax(np.array(val_acc))], 'r*', ms=12)\n",
        "plt.title('Training and Validation Accuracy, max: ' + str(np.round(val_acc[np.argmax(np.array(val_acc))],3)))\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "#fcn만 설계"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXyGT86-PFGS"
      },
      "source": [
        "### Fine tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2T-VYjgFAug"
      },
      "source": [
        "# Unfreeze all layers in base model\n",
        "tf.keras.backend.clear_session()  # For memory\n",
        "TL_base.trainable = True #TL_base:convolution 단계 -> 훈련 대상으로 변경"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUlpDXkJCkXU"
      },
      "source": [
        "# Let's take a look to see how many layers are in the base model\n",
        "print(\"Number of layers in the base model: \", len(TL_base.layers))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rr5QROAwaXBu"
      },
      "source": [
        "# Fine-tune from this layer onwards\n",
        "fine_tune_at = 107  # 81, 107 for mobilenetV2\n",
        "\n",
        "# Freeze all the layers before the `fine_tune_at` layer #내가 훈련시키는 위치 설정 기준 \n",
        "for layer in TL_base.layers[:fine_tune_at]:\n",
        "  layer.trainable =  False #107 이전 파라미터는 frozen(고정)\n",
        "\n",
        "#convnet&fcn 그림 보기(pdf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7BPKTX4KL3x"
      },
      "source": [
        "# Compile model - model: epoch 42까지 훈련시킨 모델 -> 107이후부터는 내가 훈련시킨 모델로(original과의 차이점)\n",
        "# 주의\n",
        "model.compile(optimizer='SGD', #최적화\n",
        "                loss='sparse_categorical_crossentropy',  # sparse_categorical_crossentropy - 시간 단축\n",
        "                metrics = ['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b00rS8c3KL3x"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyj71QuFKL3x"
      },
      "source": [
        "mc_callback = callbacks.ModelCheckpoint(filepath=\"./model/cifar10_pct100_mobileNetV2S_FT_SGD_best_weights.{epoch:03d}-{val_accuracy:.4f}.hdf5\", \n",
        "                              monitor='val_accuracy', verbose=0, save_best_only=True)\n",
        "es_callback = callbacks.EarlyStopping(monitor='val_accuracy', \n",
        "                            mode='max', verbose=1, patience=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMZpQzr6nbbH"
      },
      "source": [
        "## Train the fine-tuned model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zlahm1K_KL3x"
      },
      "source": [
        "INITIAL_EPOCHS = len(loss) #42개(epoch)\n",
        "FINE_TUNE_EPOCHS = 100\n",
        "TOTAL_EPOCHS = INITIAL_EPOCHS + FINE_TUNE_EPOCHS #43부터 학습 시작"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SvjuIIPKL3y"
      },
      "source": [
        "%%time\n",
        "history_fine = model.fit(x_train, y_train, \n",
        "                    epochs=TOTAL_EPOCHS, \n",
        "                    initial_epoch=INITIAL_EPOCHS, #앞의 모델에서 42번까지 학습, 앞에서 한 42번 학습까지는 그대로 유지 그 이후부터 학습\n",
        "                    batch_size=50, \n",
        "                    validation_data=(x_test, y_test),\n",
        "                    callbacks=[mc_callback,es_callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsgafpYZewb8"
      },
      "source": [
        "model.evaluate(x_test, y_test, batch_size=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QHXs-zaETkm"
      },
      "source": [
        "## Display training curve"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Iu6xftljN9t"
      },
      "source": [
        "# Display training curve\n",
        "\n",
        "acc = history.history['accuracy'] + history_fine.history['accuracy'] #튜닝 전+후\n",
        "val_acc = history.history['val_accuracy'] + history_fine.history['val_accuracy']\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "\n",
        "plt.plot(acc, label='Train accuracy')\n",
        "plt.plot(val_acc, label='Val accuracy')\n",
        "# plt.ylim([0.8, 1])\n",
        "plt.plot([INITIAL_EPOCHS-1, INITIAL_EPOCHS-1], plt.ylim(ymin=acc[0]), label='Start Fine Tuning') #41-직선, 42부터 fine tuning 시작\n",
        "plt.title(\"Fine-tune a Pretrained ConvNet:MobileNetV2-S\")\n",
        "plt.legend(loc='upper left')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppeaO5hxKL3y"
      },
      "source": [
        "# More graphs of loss and accuracy in Fine Tuning - 43부터 그림, 90%로 정확도 높아짐\n",
        "history_dict = history_fine.history\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(INITIAL_EPOCHS, INITIAL_EPOCHS+len(loss))\n",
        "\n",
        "plt.figure(figsize=(14, 4))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(epochs, loss, 'go-', label='Training Loss')\n",
        "plt.plot(epochs, val_loss, 'bd', label='Validation Loss')\n",
        "plt.plot(INITIAL_EPOCHS + np.argmin(np.array(val_loss))+1,val_loss[np.argmin(np.array(val_loss))], 'r*', ms=12)\n",
        "plt.title('Training and Validation Loss, min: ' + str(np.round(val_loss[np.argmin(np.array(val_loss))],3)))\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "acc = history_dict['accuracy']\n",
        "val_acc = history_dict['val_accuracy']\n",
        "\n",
        "# epochs = range(1, len(loss_values) + 1)\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(epochs, acc, 'go-', label='Training Accuracy')#, c='blue')\n",
        "plt.plot(epochs, val_acc, 'bd', label='Validation Accuracy') #, c='red')\n",
        "plt.plot(INITIAL_EPOCHS + np.argmax(np.array(val_acc))+1,val_acc[np.argmax(np.array(val_acc))], 'r*', ms=12)\n",
        "plt.title('Fine Tuning: Training and Validation Accuracy, max: ' + str(np.round(val_acc[np.argmax(np.array(val_acc))],3)))\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKWvz32CKL3y"
      },
      "source": [
        "### Visualize predictions\n",
        "We can take a look at the predictions on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pI-c0pHMKL3y",
        "trusted": true
      },
      "source": [
        "#최종 모델로 테스트 데이터에서 예측\n",
        "probabilities = model.predict(x_test, batch_size=50)\n",
        "probabilities = np.argmax(probabilities, axis = 1)\n",
        "\n",
        "display_images(testX, probabilities, testy, \"Predictions of the validation data\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lZcO6W8KL3y"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRc4TUHFETkn"
      },
      "source": [
        "tf.keras.backend.clear_session()  # For memory"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCd3ub5eETkn"
      },
      "source": [
        "---"
      ]
    }
  ]
}